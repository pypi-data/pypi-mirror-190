from __future__ import annotations
from operator import itemgetter
from pathlib import Path

from typing import Iterable, Literal, Optional, Union, overload
from grams.algorithm.literal_matchers.literal_match import LiteralMatch
from grams.algorithm.literal_matchers.text_parser import TextParser
from hugedict.prelude import HugeMutableMapping
from kgdata.wikidata.db import WikidataDB
from kgdata.wikidata.extra_ent_db import get_multilingual_key
from kgdata.wikidata.models.wdclass import WDClass
from kgdata.wikidata.models.wdentity import WDEntity
from loguru import logger
from ned.actors.candidate_generation import CanGenActor
from ned.actors.dataset import DatasetActor
from ned.candidate_ranking.cr_method import CandidateRankingMethod
from ned.candidate_ranking.helpers.dataset import MyDatasetDict
from ned.candidate_ranking.helpers.db_helper import _gather_entities_attr
from ned.data_models.npmodels import ColumnIndex
import numpy as np
import ray
from nptyping import Float64, Int32, NDArray, Object, Shape
from ream.actor_state import ActorState
from ream.actors.base import BaseActor
from ream.cache_helper import Cache, Cacheable, unwrap_cache_decorators
from ream.data_model_helper import NumpyDataModel

from ned.data_models.prelude import DatasetCandidateEntities, DatasetIndex
from ned.data_models.pymodels import NEDExample
from ream.dataset_helper import DatasetQuery
from ream.helper import orjson_dumps
from ream.params_helper import NoParams
from slugify import slugify
from sm.misc.funcs import batch
from sm.misc.ray_helper import get_instance, ray_map
import torch
from tqdm import tqdm
import rltk.similarity as sim


class CRDatasetEnt(NumpyDataModel):
    # fmt: off
    __slots__ = [
        "index", "cell", "cell_id", 
        "table_id", "col_index", "row_index", 
        "entity_id", "entity_label", "entity_description", 
        "entity_aliases", "entity_popularity"
    ]
    # fmt: on
    # mapping from table_id => col_index => row_index => (start, end)
    index: dict[str, dict[int, dict[int, tuple[int, int]]]]
    cell: NDArray[Shape["*"], Object]  # content of the cell
    # unique number for each cell across all tables
    cell_id: NDArray[Shape["*"], Int32]
    table_id: NDArray[Shape["*"], Object]  # table id of the cell
    col_index: NDArray[Shape["*"], Int32]  # column index of the cell
    row_index: NDArray[Shape["*"], Int32]  # row index of the cell
    entity_id: NDArray[Shape["*"], Object]
    entity_label: NDArray[Shape["*"], Object]
    entity_description: NDArray[Shape["*"], Object]
    entity_aliases: NDArray[Shape["*"], Object]
    entity_popularity: NDArray[Shape["*"], Object]

    @staticmethod
    def create(examples: list[NEDExample], candidates: DatasetCandidateEntities):
        index = {}
        cell = []
        cell_id = []
        table_id = []
        col_index = []
        row_index = []
        entity_id = []
        entity_label = []
        entity_description = []
        entity_aliases = []
        entity_popularity = []

        cell_index_offset = 0

        for example in tqdm(examples, desc="make base ent"):
            tid = example.table.table_id
            tindex = candidates.index[tid][-1]
            index[tid] = {}
            nrows, ncols = example.table.shape()
            for ci in tindex:
                cindex = tindex[ci][-1]
                index[tid][ci] = {}
                for ri in cindex:
                    cell_index = cell_index_offset + ri * ncols + ci
                    rowstart = len(cell)
                    link = example.cell_links[ri, ci]
                    text = example.table[ri, ci]
                    if link is not None:
                        for ent in link.entities:
                            cell.append(text)
                            cell_id.append(cell_index)
                            table_id.append(tid)
                            col_index.append(ci)
                            row_index.append(ri)
                            entity_id.append(ent.id)
                            entity_label.append(ent.label)
                            entity_description.append(ent.description)
                            entity_aliases.append(ent.aliases)
                            entity_popularity.append(ent.popularity)
                    index[tid][ci][ri] = (rowstart, len(cell))
            cell_index_offset += nrows * ncols

        return CRDatasetEnt(
            index=index,
            cell=np.asarray(cell, dtype=np.object_),
            cell_id=np.asarray(cell_id, dtype=np.int32),
            table_id=np.asarray(table_id, dtype=np.object_),
            col_index=np.asarray(col_index, dtype=np.int32),
            row_index=np.asarray(row_index, dtype=np.int32),
            entity_id=np.asarray(entity_id, dtype=np.object_),
            entity_label=np.asarray(entity_label, dtype=np.object_),
            entity_description=np.asarray(entity_description, dtype=np.object_),
            entity_aliases=np.asarray(entity_aliases, dtype=np.object_),
            entity_popularity=np.array(entity_popularity, dtype=np.float64),
        )


class CRDatasetCan(NumpyDataModel):
    # fmt: off
    __slots__ = [
        "index", "cell_id", "table_index", "col_index", 
    ]
    # fmt: on
    # the order of this object and candidates are the same
    index: DatasetIndex
    cell_id: NDArray[Shape["*"], Int32]
    table_index: NDArray[Shape["*"], Int32]
    col_index: NDArray[Shape["*"], Int32]

    @staticmethod
    def create(examples: list[NEDExample], candidates: DatasetCandidateEntities):
        cell_id = []
        table_index = []
        col_index = []

        cell_index_offset = 0
        # using a pointer to make sure that the order of this object and candidates are the same
        pointer = 0

        for ti, example in tqdm(enumerate(examples), desc="make base can"):
            tid = example.table.table_id
            tindex = candidates.index[tid][-1]
            nrows, ncols = example.table.shape()
            for ci in tindex:
                cindex = tindex[ci][-1]
                for ri, (rstart, rend) in cindex.items():
                    if rstart != pointer:
                        # as long as candidates.index maintaining the order, this should not happen
                        raise ValueError(
                            "The order of candidates and examples are not the same"
                        )
                    # the order is matched, move the pointer forward
                    pointer = rend
                    cell_index = cell_index_offset + ri * ncols + ci
                    cans = candidates.get_cell_candidates(tid, ri, ci)
                    cell_id.extend([cell_index] * len(cans))
                    table_index.extend([ti] * len(cans))
                    col_index.extend([ci] * len(cans))
            cell_index_offset += nrows * ncols

        return CRDatasetCan(
            index=candidates.index,
            cell_id=np.asarray(cell_id, dtype=np.int32),
            table_index=np.asarray(table_index, dtype=np.int32),
            col_index=np.asarray(col_index, dtype=np.int32),
        )


class CRDatasetEntFeatures(NumpyDataModel):
    __slots__ = ["entity_features"]
    entity_features: NDArray[Shape["*,*"], Float64]

    @staticmethod
    def create(baseent: CRDatasetEnt):
        @ray.remote
        def step1_extract_ent_features(
            cells: list[str],
            ents_label: list[str],
            ents_popularity: list[float],
        ):
            feats = []

            for i in range(len(cells)):
                feat = _extract_pairwise_features_v1(cells[i], ents_label[i])
                feat.append(ents_popularity[i])
                feats.append(feat)

            return np.asarray(feats, dtype=np.float64)

        entity_features = ray_map(
            step1_extract_ent_features.remote,
            batch(512, baseent.cell, baseent.entity_label, baseent.entity_popularity),
            verbose=True,
            desc="extract entity features",
        )
        return CRDatasetEntFeatures(
            np.concatenate([x for x in entity_features if x.shape[0] > 0], axis=0)
        )


class CRDatasetCanFeatures(NumpyDataModel):
    __slots__ = ["candidate_features"]
    # the order of this object and candidates are the same
    candidate_features: NDArray[Shape["*,*"], Float64]

    @staticmethod
    def create(examples: list[NEDExample], candidates: DatasetCandidateEntities):
        @ray.remote
        def step1_extract_can_features(
            cell: str, cans_label: np.ndarray, cans_popularity: np.ndarray
        ):
            feats = []

            for i in range(len(cans_label)):
                feat = _extract_pairwise_features_v1(cell, cans_label[i])
                feat.append(cans_popularity[i])
                feats.append(feat)

            return np.asarray(feats, dtype=np.float64)

        args = []

        # using a pointer to make sure that the order of this object and candidates are the same
        pointer = 0
        for example in tqdm(examples, desc="make base can"):
            tid = example.table.table_id
            tindex = candidates.index[tid][-1]
            for ci in tindex:
                cindex = tindex[ci][-1]
                for ri, (rstart, rend) in cindex.items():
                    if rstart != pointer:
                        # as long as candidates.index maintaining the order, this should not happen
                        raise ValueError(
                            "The order of candidates and examples are not the same"
                        )
                    # the order is matched, move the pointer forward
                    pointer = rend
                    cell = example.table[ri, ci]
                    cans = candidates.get_cell_candidates(tid, ri, ci)
                    args.append((cell, cans.label, cans.popularity))

        candidate_features = ray_map(
            step1_extract_can_features.remote,
            args,
            verbose=True,
            desc="extract candidate features",
        )
        return CRDatasetCanFeatures(
            np.concatenate([x for x in candidate_features if x.shape[0] > 0], axis=0)
        )


class CRDatasetCanTypes(NumpyDataModel):
    __slots__ = ["candidate_types"]
    # the order of this object is the same as candidates
    # each item is a list of numbers, for wikidata entities, it is the id without Q
    candidate_types: NDArray[Shape["*"], Object]

    @staticmethod
    def create(candidates: DatasetCandidateEntities):
        @ray.remote
        def gather_encoded_instanceof(database_dir, col_ents):
            id2types: dict[str, list[str]] = _gather_entities_attr(
                database_dir, set(col_ents), "instanceof"
            )
            type2index = {}
            for types in id2types.values():
                for t in types:
                    if t not in type2index:
                        type2index[t] = CRDatasetCanTypes.encoded_type(t)

            return np.array(
                [[type2index[t] for t in id2types[eid]] for eid in col_ents],
                dtype=np.object_,
            )

        # using a pointer to make sure that the order of this object and candidates are the same
        pointer = 0
        args = []
        db_dir_ref = ray.put(WikidataDB.get_instance().database_dir)
        for tstart, tend, tindex in candidates.index.values():
            for cstart, cend, cindex in tindex.values():
                if cstart != pointer:
                    # as long as candidates.index maintaining the order, this should not happen
                    raise ValueError(
                        "The order of candidates and examples are not the same"
                    )
                # the order is matched, move the pointer forward
                pointer = cend
                args.append((db_dir_ref, candidates.id[cstart:cend]))

        candidate_types = ray_map(
            gather_encoded_instanceof.remote,
            args,
            verbose=True,
            desc="extract candidate types",
        )

        return CRDatasetCanTypes(
            np.concatenate([x for x in candidate_types if x.shape[0] > 0], axis=0)
        )

    @staticmethod
    def encoded_type(type: str):
        assert type.startswith("Q") and type[1:].isdigit()
        index = int(type[1:])
        assert index < 2000000000
        return index


class CRDatasetExtendedCanTypes(NumpyDataModel):
    __slots__ = ["candidate_types"]
    # the order of this object is the same as candidates
    # each item is a list of numbers, for wikidata entities, it is the id without Q
    candidate_types: NDArray[Shape["*"], Object]

    @staticmethod
    def create(examples: list[NEDExample], candidates: DatasetCandidateEntities):
        @ray.remote
        def gather_encoded_instanceof(database_dir, col_ents, col_types: set[str]):
            from ned.candidate_generation.oracle_semtyper import _get_class_ancestors

            # TODO: fix me, cause currently we can't get the child types of a type yet.
            id2types: dict[str, list[str]] = _gather_entities_attr(
                database_dir, set(col_ents), "instanceof"
            )
            wdclasses: HugeMutableMapping[str, WDClass] = get_instance(
                lambda: WikidataDB(database_dir).wdclasses,
                f"kgdata.wikidata.db[{database_dir}]",
            ).cache()

            parent_coltypes = _get_class_ancestors(wdclasses, col_types, 1)[1]

            for id, types in id2types.items():
                newtypes = set(types)
                if len(parent_coltypes.intersection(types)) > 0:
                    newtypes.update(col_types)

                cantype_levels = _get_class_ancestors(wdclasses, newtypes, 2)
                for level in cantype_levels:
                    newtypes.update(level)

                id2types[id] = list(newtypes)

            type2index = {}
            for types in id2types.values():
                for t in types:
                    if t not in type2index:
                        type2index[t] = CRDatasetCanTypes.encoded_type(t)

            return np.array(
                [[type2index[t] for t in id2types[eid]] for eid in col_ents],
                dtype=np.object_,
            )

        # using a pointer to make sure that the order of this object and candidates are the same
        pointer = 0
        args = []
        db_dir_ref = ray.put(WikidataDB.get_instance().database_dir)
        for example in examples:
            tstart, tend, tindex = candidates.index[example.table.table_id]
            for ci, ctypes in zip(example.entity_columns, example.entity_column_types):
                cstart, cend, cindex = tindex[ci]
                if cstart != pointer:
                    # as long as candidates.index maintaining the order, this should not happen
                    raise ValueError(
                        "The order of candidates and examples are not the same"
                    )
                # the order is matched, move the pointer forward
                pointer = cend
                args.append(
                    (
                        db_dir_ref,
                        candidates.id[cstart:cend],
                        {ctype.id for ctype in ctypes},
                    )
                )

        candidate_types = ray_map(
            gather_encoded_instanceof.remote,
            args,
            verbose=True,
            desc="extract candidate types",
        )

        return CRDatasetExtendedCanTypes(
            np.concatenate([x for x in candidate_types if x.shape[0] > 0], axis=0)
        )

    @staticmethod
    def encoded_type(type: str):
        assert type.startswith("Q") and type[1:].isdigit()
        index = int(type[1:])
        assert index < 2000000000
        return index


class CRDatasetEntMatchedProps(NumpyDataModel):
    __slots__ = ["ent_matched_props"]
    # the order of this object is the same as candidates
    # each item is a list of tuple of (prop id, column id, score)
    ent_matched_props: NDArray[Shape["*,*"], Object]

    @staticmethod
    def create(examples: list[NEDExample], baseent: CRDatasetEnt):
        @ray.remote
        def match_props(
            database_dir,
            example: NEDExample,
            column_index: int,
            column_ent_ids: NDArray[Shape["*"], Object],
            cindex: list[tuple[int, tuple[int, int]]],
        ):
            # gather all the entities in this column
            db = get_instance(
                lambda: WikidataDB(database_dir).wdentities,
                f"ned.candidate_ranking.dataset.wdentities",
            ).cache()
            textparser = TextParser()
            literal_match = LiteralMatch(db)

            other_columns = [
                col for j, col in enumerate(example.table.columns) if j != column_index
            ]

            output = []
            for ri, (rstart, rend) in cindex:
                can_ids = column_ent_ids[rstart:rend]
                for can_id in can_ids:
                    can = db[can_id]
                    can_matches = []
                    if len(other_columns) == 0:
                        can_matches.append((None, None, 0.0))
                    else:
                        for col in other_columns:
                            value = col.values[ri]
                            parsed_value = textparser.parse(value)
                            matches = CRDatasetCanMatchedProps.value_search(
                                can, literal_match, parsed_value
                            )
                            if len(matches) == 0:
                                can_matches.append((None, None, 0.0))
                            else:
                                propid, score = max(matches, key=itemgetter(1))
                                can_matches.append((propid, col.index, score))
                    can_matched_score = [x[2] for x in can_matches]
                    output.append([np.sum(can_matched_score)])

            return np.array(output)

        example_refs = {ex.table.table_id: ray.put(ex) for ex in examples}
        db_dir_ref = ray.put(WikidataDB.get_instance().database_dir)
        args = []
        for tid, tindex in baseent.index.items():
            for ci, cindex in tindex.items():
                cindex = list(cindex.items())
                if len(cindex) == 0:
                    continue
                cstart = cindex[0][1][0]
                cend = cindex[-1][1][1]
                cindex = [
                    (ri, (rstart - cstart, rend - cstart))
                    for ri, (rstart, rend) in cindex
                ]
                args.append(
                    (
                        db_dir_ref,
                        example_refs[tid],
                        ci,
                        baseent.entity_id[cstart:cend],
                        cindex,
                    )
                )

        resp = ray_map(
            match_props.remote,
            args,
            verbose=True,
            desc="matching properties of entities with other columns",
        )
        arr = np.concatenate([x for x in resp if x.shape[0] > 0], axis=0)
        assert arr.shape[0] == len(baseent)
        return CRDatasetEntMatchedProps(arr)


class CRDatasetCanMatchedProps(NumpyDataModel):
    __slots__ = ["candidate_matched_props"]
    # the order of this object is the same as candidates
    # each item is a list of tuple of (prop id, column id, score)
    candidate_matched_props: NDArray[Shape["*,*"], Object]

    @staticmethod
    def create(examples: list[NEDExample], candidates: DatasetCandidateEntities):
        @ray.remote
        def match_props(
            database_dir,
            example: NEDExample,
            column_index: int,
            column_can_ids: NDArray[Shape["*"], Object],
            cindex: list[tuple[int, tuple[int, int]]],
        ):
            # gather all the entities in this column
            db = get_instance(
                lambda: WikidataDB(database_dir).wdentities,
                f"ned.candidate_ranking.dataset.wdentities",
            ).cache()
            textparser = TextParser()
            literal_match = LiteralMatch(db)

            other_columns = [
                col for j, col in enumerate(example.table.columns) if j != column_index
            ]
            output = []
            for ri, (rstart, rend) in cindex:
                can_ids = column_can_ids[rstart:rend]
                for can_id in can_ids:
                    can = db[can_id]
                    can_matches = []
                    for col in other_columns:
                        value = col.values[ri]
                        parsed_value = textparser.parse(value)
                        matches = CRDatasetCanMatchedProps.value_search(
                            can, literal_match, parsed_value
                        )
                        if len(matches) == 0:
                            can_matches.append((None, None, 0.0))
                        else:
                            propid, score = max(matches, key=itemgetter(1))
                            can_matches.append((propid, col.index, score))

                    can_matched_score = [x[2] for x in can_matches]
                    output.append([np.sum(can_matched_score)])

            return np.array(output)

        example_refs = {ex.table.table_id: ray.put(ex) for ex in examples}
        db_dir_ref = ray.put(WikidataDB.get_instance().database_dir)
        args = []
        for tid, (tstart, tend, tindex) in candidates.index.items():
            for ci, (cstart, cend, cindex) in tindex.items():
                cindex = [
                    (ri, (rstart - cstart, rend - cstart))
                    for ri, (rstart, rend) in cindex.items()
                ]
                args.append(
                    (
                        db_dir_ref,
                        example_refs[tid],
                        ci,
                        candidates.id[cstart:cend],
                        cindex,
                    )
                )

        resp = ray_map(
            match_props.remote,
            args,
            verbose=True,
            desc="matching properties of candidates with other columns",
        )
        arr = np.concatenate([x for x in resp if x.shape[0] > 0], axis=0)
        assert arr.shape[0] == len(candidates)
        return CRDatasetCanMatchedProps(arr)

    @staticmethod
    def value_search(ent: WDEntity, literal_match: LiteralMatch, value):
        """Find property/qualifier of an entity that matches the value. If the match one is qualifier, prepend with the property id.

        For now, this function hasn't considered matched qualifiers.
        """
        matches = []
        for p, stmts in ent.props.items():
            if p == "P31":
                # no need to search in the instanceOf property, as the ontology is removed from the databased as they are huge
                continue

            for stmt_i, stmt in enumerate(stmts):
                has_stmt_value = False
                for fn, (match, confidence) in literal_match.match(
                    stmt.value, value, skip_unmatch=True
                ):
                    matches.append((p, confidence))
                    has_stmt_value = True

                for q, qvals in stmt.qualifiers.items():
                    for qval in qvals:
                        for fn, (match, confidence) in literal_match.match(
                            qval, value, skip_unmatch=True
                        ):
                            matches.append((f"{p}/{q}", confidence))
        return matches


CRDatasetEnt.init()
CRDatasetCan.init()
CRDatasetEntFeatures.init()
CRDatasetCanFeatures.init()
CRDatasetCanTypes.init()
CRDatasetExtendedCanTypes.init()
CRDatasetEntMatchedProps.init()
CRDatasetCanMatchedProps.init()


class CRDataset(Cacheable):
    def prep(self):
        if not ray.is_initialized():
            ray.init()

    @Cache.cls(CRDatasetEnt, "parq").file(cache_args=[], mem_persist=True)
    def base_ent(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ):
        self.prep()
        return CRDatasetEnt.create(examples, candidates)

    @Cache.cls(CRDatasetCan, "parq").file(cache_args=[], mem_persist=True)
    def base_can(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ):
        self.prep()
        return CRDatasetCan.create(examples, candidates)

    @Cache.cls(CRDatasetEntFeatures, "parq").file(cache_args=[], mem_persist=True)
    def ent_features(self, crents: CRDatasetEnt):
        self.prep()
        return CRDatasetEntFeatures.create(crents)

    @Cache.cls(CRDatasetCanFeatures, "parq").file(cache_args=[], mem_persist=True)
    def can_features(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ):
        self.prep()
        return CRDatasetCanFeatures.create(examples, candidates)

    @Cache.cls(CRDatasetCanTypes, "parq").file(cache_args=[], mem_persist=True)
    def can_types(self, candidates: DatasetCandidateEntities):
        self.prep()
        return CRDatasetCanTypes.create(candidates)

    @Cache.cls(CRDatasetExtendedCanTypes, "parq").file(cache_args=[], mem_persist=True)
    def extended_can_types(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ):
        self.prep()
        return CRDatasetExtendedCanTypes.create(examples, candidates)

    @Cache.cls(CRDatasetEntMatchedProps, "parq").file(cache_args=[], mem_persist=True)
    def ent_matched_props(self, examples: list[NEDExample], base_ent: CRDatasetEnt):
        self.prep()
        return CRDatasetEntMatchedProps.create(examples, base_ent)

    @Cache.cls(CRDatasetCanMatchedProps, "parq").file(cache_args=[], mem_persist=True)
    def can_matched_props(
        self, examples: list[NEDExample], candidates: DatasetCandidateEntities
    ):
        self.prep()
        return CRDatasetCanMatchedProps.create(examples, candidates)

    def columnwise_1(
        self,
        candidates: DatasetCandidateEntities,
        crents: CRDatasetEnt,
        crcans: CRDatasetCan,
        crcan_features: CRDatasetCanFeatures,
        crent_features: CRDatasetEntFeatures,
        crcan_types: Union[CRDatasetCanTypes, CRDatasetExtendedCanTypes],
        crent_types: Union[CRDatasetEntTypes, CRDatasetExtendedEntTypes],
        crcan_matched_props: Optional[CRDatasetCanMatchedProps] = None,
        crent_matched_props: Optional[CRDatasetEntMatchedProps] = None,
        add_missing_gold: Literal["no", "singleonly", "multiple"] = "no",
        top_k: Optional[int] = 100,
    ):
        out_features = []
        out_labels = []
        out_cells = []
        out_types = []
        out_masks = []
        out_can_idx = []  # to keep track of the candidate ids for later reconstruction

        if top_k is None:
            # find the max number of candidates
            ndsize = 0
            for tstart, tend, tindex in candidates.index.values():
                for cstart, cend, cindex in tindex.values():
                    ndsize = max(
                        ndsize, max(end - start for start, end in cindex.values())
                    )
        else:
            ndsize = top_k

        for tbl, (tstart, tend, tindex) in tqdm(
            candidates.index.items(), desc="columnwise dataset: prep"
        ):
            for ci, (cstart, cend, cindex) in tindex.items():
                col_features = []
                col_labels = []
                col_cells = []
                col_types = []
                col_masks = []
                col_can_idx = []

                for ri, (start, end) in cindex.items():
                    s, e = crents.index[tbl][ci][ri]
                    entities_id = crents.entity_id[s:e]
                    candidates_id = candidates.id[start:end]
                    features = crcan_features.candidate_features[start:end]

                    can_scores = (
                        np.sum(features[:, :-1], axis=1) + features[:, -1] * 20
                    ) / features.shape[1]
                    sortedindex = np.argsort(can_scores, kind="stable")[::-1]
                    if top_k is not None:
                        sortedindex = sortedindex[:top_k]

                    tmp_feats = features[sortedindex]
                    if crcan_matched_props is not None:
                        extra_features = crcan_matched_props.candidate_matched_props[
                            start:end
                        ][sortedindex]
                        tmp_feats = np.concatenate([tmp_feats, extra_features], axis=1)

                    tmp_labels = np.isin(
                        candidates_id[sortedindex], entities_id
                    ).astype(np.uint8)
                    tmp_ids = crcans.cell_id[start:end][sortedindex]
                    tmp_types = crcan_types.candidate_types[start:end][sortedindex]
                    tmp_masks = np.ones((ndsize,), dtype=bool)
                    tmp_can_idx = sortedindex + start
                    if tmp_feats.shape[0] < ndsize:
                        pad_size = (0, ndsize - tmp_feats.shape[0])
                        tmp_masks[tmp_feats.shape[0] :] = 0
                        tmp_feats = np.pad(
                            tmp_feats,
                            (pad_size, (0, 0)),  # type: ignore
                        )
                        tmp_labels = np.pad(tmp_labels, pad_size)
                        tmp_ids = np.pad(tmp_ids, pad_size)
                        tmp_types = list(tmp_types)
                        tmp_can_idx = np.pad(tmp_can_idx, pad_size)
                        for _ in range(pad_size[1]):
                            tmp_types.append([])

                    col_features.append(tmp_feats)
                    col_labels.append(tmp_labels)
                    col_cells.append(tmp_ids)
                    col_types.append(tmp_types)
                    col_masks.append(tmp_masks)
                    col_can_idx.append(tmp_can_idx)

                out_features.append(np.stack(col_features))
                out_labels.append(np.stack(col_labels))
                out_cells.append(np.stack(col_cells))
                out_types.append(col_types)
                out_masks.append(np.stack(col_masks))
                out_can_idx.append(np.stack(col_can_idx))

        if top_k is None:
            assert sum(col_masks.sum() for col_masks in out_masks) == len(candidates)

        # transform col types into a vector multi-hot encoding
        # this is not an ideal way to do it, but we temporary doing it here as we still use huggingface datasets
        n_types = max(
            max(len(cell_types) for cell_types in col_types) for col_types in out_types
        )
        # assert n_types <= 16

        new_out_types = []
        type_encoding = []

        for col_types in tqdm(
            out_types, desc="columnwise dataset: encoding candidate types"
        ):
            # remap the index
            type2index = {}
            new_types = []
            for i, cell_types in enumerate(col_types):
                lst1 = []
                for can_types in cell_types:
                    lst2 = []
                    for t in can_types:
                        if t not in type2index:
                            type2index[t] = len(type2index)
                        lst2.append(type2index[t])
                    lst1.append(np.array(lst2, dtype=np.uint8))
                new_types.append(lst1)

            new_col_types = np.zeros(
                (len(new_types), len(new_types[0]), len(type2index)), dtype=np.int32
            )
            for i, cell_types in enumerate(new_types):
                for j, can_types in enumerate(cell_types):
                    new_col_types[i, j, can_types] = 1

            type_encoding.append(np.array(list(type2index), dtype=np.int32))
            new_out_types.append(new_col_types)

        return {
            "features": out_features,
            "label": out_labels,
            "cell": out_cells,
            "can_idx": out_can_idx,
            "mask": out_masks,
            "types": new_out_types,
            "type_encoding": type_encoding,
        }

    def pairwise_1(
        self,
        candidates: DatasetCandidateEntities,
        crents: CRDatasetEnt,
        crcans: CRDatasetCan,
        crent_features: CRDatasetEntFeatures,
        crcan_features: CRDatasetCanFeatures,
        crent_matched_props: Optional[CRDatasetEntMatchedProps] = None,
        crcan_matched_props: Optional[CRDatasetCanMatchedProps] = None,
        add_missing_gold: Literal["no", "singleonly", "multiple"] = "no",
        top_k: Optional[int] = 100,
    ):
        out_labels = []
        out_features = []
        out_cells = []

        for tbl, (tstart, tend, tindex) in tqdm(
            candidates.index.items(), desc="pairwise dataset"
        ):
            for ci, (cstart, cend, cindex) in tindex.items():
                for ri, (start, end) in cindex.items():
                    s, e = crents.index[tbl][ci][ri]
                    entities_id = crents.entity_id[s:e]

                    candidates_id = candidates.id[start:end]
                    features = crcan_features.candidate_features[start:end]
                    cells_id = crcans.cell_id[start:end]

                    if top_k is not None:
                        can_scores = np.mean(features[:, :-2], axis=1)
                        sortedindex = np.argsort(can_scores, kind="stable")[::-1]
                        sortedindex = sortedindex[:top_k]

                        topklabels = np.isin(
                            candidates_id[sortedindex], entities_id
                        ).astype(np.uint8)
                        features = features[sortedindex]
                        cells_id = cells_id[sortedindex]

                        if crcan_matched_props is not None:
                            extra_features = (
                                crcan_matched_props.candidate_matched_props[start:end][
                                    sortedindex
                                ]
                            )
                            features = np.concatenate(
                                [features, extra_features], axis=1
                            )
                    else:
                        topklabels = np.isin(candidates_id, entities_id).astype(
                            np.uint8
                        )

                        if crcan_matched_props is not None:
                            extra_features = (
                                crcan_matched_props.candidate_matched_props[start:end]
                            )
                            features = np.concatenate(
                                [features, extra_features], axis=1
                            )

                    out_labels.extend(topklabels)
                    out_features.extend(features)
                    out_cells.extend(cells_id)

                    if add_missing_gold != "no" and topklabels.sum() == 0:
                        if add_missing_gold == "singleonly" and len(entities_id) == 1:
                            out_labels.append(1)
                            ent_features = crent_features.entity_features[s]
                            if crent_matched_props is not None:
                                ent_features = np.concatenate(
                                    [
                                        ent_features,
                                        crent_matched_props.ent_matched_props[s],
                                    ]
                                )

                            out_features.append(ent_features)
                            out_cells.append(crents.cell_id[s])
                        elif add_missing_gold == "multiple":
                            out_labels.extend([1] * len(entities_id))
                            ent_features = crent_features.entity_features[s:e]
                            if crent_matched_props is not None:
                                ent_features = np.concatenate(
                                    [
                                        ent_features,
                                        crent_matched_props.ent_matched_props[s:e],
                                    ],
                                    axis=1,
                                )
                            out_features.extend(ent_features)
                            out_cells.extend(crents.cell_id[s:e])

        return {"label": out_labels, "features": out_features, "cell": out_cells}

    def contrastive_pairwise_1(
        self,
        candidates: DatasetCandidateEntities,
        crents: CRDatasetEnt,
        crcans: CRDatasetCan,
        crcan_features: CRDatasetCanFeatures,
        triplet_format: bool,
    ):
        """
        Args:
            triplet_format: if True, return a triplet format (pos & neg pairs)
        """
        if triplet_format and len(crents) == 0:
            raise Exception(
                "We want to return a triplet format, but there is no label provided so we cannot"
                "determine which candidate is positive and which is negative"
            )

        if not triplet_format:
            out_cell = []
            out_label = []
            out_pos_features = []
            for tid, (tstart, tend, tindex) in tqdm(
                candidates.index.items(), desc="contrastive pairwise"
            ):
                for ci, (cstart, cend, cindex) in tindex.items():
                    for ri, (start, end) in cindex.items():
                        s, e = crents.index[tid][ci][ri]
                        entities_id = crents.entity_id[s:e]

                        cells_id = crcans.cell_id[start:end]
                        features = crcan_features.candidate_features[start:end]

                        label = np.isin(candidates.id[start:end], entities_id).astype(
                            np.uint8
                        )

                        out_cell.extend(cells_id)
                        out_label.extend(label)
                        out_pos_features.extend(features)

            return {
                "label": out_label,
                "cell": out_cell,
                "pos_features": out_pos_features,
            }

        outs = []
        for tid, (tstart, tend, tindex) in tqdm(
            candidates.index.items(), desc="contrastive pairwise"
        ):
            for ci, (cstart, cend, cindex) in tindex.items():
                for ri, (start, end) in cindex.items():
                    s, e = crents.index[tid][ci][ri]
                    if s == e:
                        # there is no gold entity for this cell and thus no positive pair
                        # because we haven't implemented NIL entity yet
                        # TODO: handle NIL entity here
                        continue
                    entities_id = crents.entity_id[s:e]

                    features = crcan_features.candidate_features[start:end]
                    label = np.isin(candidates.id[start:end], entities_id).astype(
                        np.uint8
                    )

                    pos_features = features[label == 1]
                    neg_features = features[label == 0]

                    if len(neg_features) == 0:
                        # no negative examples
                        logger.debug(
                            "no negative examples found for table {}, row {} and col {}",
                            tid,
                            ri,
                            ci,
                        )
                        continue

                    for i in range(pos_features.shape[0]):
                        outs.append(
                            {
                                "pos_features": pos_features[i],
                                "neg_features": neg_features,
                                "neg_size": neg_features.shape[0],
                            }
                        )
        return outs

    @staticmethod
    def contrastive_pairwise_triplet_collate_fn(batch):
        pos_features = []
        neg_features = []
        neg_size = []
        for b in batch:
            pos_features.append(torch.as_tensor(b["pos_features"]))
            neg_features.append(torch.as_tensor(b["neg_features"]))
            neg_size.append(b["neg_size"])
        pos_features = torch.stack(pos_features)
        neg_features = torch.concat(neg_features)
        neg_size = torch.tensor(neg_size)
        return {
            "pos_features": pos_features,
            "neg_features": neg_features,
            "neg_size": neg_size,
        }


class NoCacheCRDataset(CRDataset):
    def __init__(self):
        pass


unwrap_cache_decorators(NoCacheCRDataset)


# number of extracted pairwise features
# for setting default gold_features for NIL entity
# two extra features are added from pyserini score and pagerank
N_PAIRWISE_FEATURES = 4
N_EXTRA_FEATURES = 1


def _extract_pairwise_features_v1(text: str, entity_label: str):
    """Extract feature from mention and entity.

    When this function is changed, the version number should be updated in this function and all step functions
    """
    text_tokens = text.split(" ")
    entity_label_tokens = entity_label.split(" ")

    return [
        sim.levenshtein_similarity(text, entity_label),
        sim.jaro_winkler_similarity(
            text, entity_label, threshold=0.7, scaling_factor=0.1, prefix_len=4
        ),
        sim.monge_elkan_similarity(text_tokens, entity_label_tokens),
        sim.hybrid_jaccard_similarity(set(text_tokens), set(entity_label_tokens)),
    ]


class CanRankDataset(BaseActor[str, NoParams]):
    """CHANGELOG:
    - 200: switch to new ream version
    """

    VERSION = 211

    def __init__(
        self,
        cangen_actor: CanGenActor,
        dataset_actor: DatasetActor,
    ):
        super().__init__(NoParams(), dep_actors=[cangen_actor, dataset_actor])
        self.cangen_actor = cangen_actor
        self.dataset_actor = dataset_actor

    def run(
        self,
        method: CandidateRankingMethod,
        dsquery: str,
        for_training: bool = False,
        num_proc: Optional[int] = None,
    ):
        wfs = self.get_working_fs()
        dsquery_ = DatasetQuery.from_string(dsquery)
        # a build directory that is unique to the dataset query and can be shared across methods
        builddir = wfs.get(
            dsquery_.dataset + "_build", {"query": dsquery}, save_key=True
        )

        if builddir.exists():
            builddir_path = builddir.get()
        else:
            with builddir.reserve_and_track() as builddir_path:
                pass
        self.logger.debug(
            "Cache common sub-datasets to a build directory: {}", builddir_path
        )

        self.logger.debug("Retrieve candidates of dataset: {}", dsquery)
        dsdict = self.dataset_actor.run(dsquery)
        cangen_dsdict = self.cangen_actor.run_dataset(dsquery)

        self.logger.debug("Extract features of dataset: {}", dsquery)
        canrank_dsdict = MyDatasetDict(dsquery_.dataset, {})
        for split, candidates in cangen_dsdict.items():
            self.logger.debug("\tSubset: {}", split)
            examples = dsdict[split]
            canrank_dsdict[split] = method.generate_dataset(
                examples,
                candidates,
                num_proc=num_proc,
                for_training=for_training,
                cache_dir=builddir_path / split,
            )

        return canrank_dsdict

    def get_cache_key(
        self: CanRankDataset,
        method: CandidateRankingMethod,
        dsquery: str,
        for_training: bool = False,
        num_proc: Optional[int] = None,
    ):
        deps = [actor.get_actor_state() for actor in self.dep_actors]
        state = ActorState.create(
            method.__class__, method.get_generating_dataset_args(), dependencies=deps
        )
        return orjson_dumps(
            {"state": state.to_dict(), "dsquery": dsquery, "for_training": for_training}
        )

    def get_cache_filename(
        self: CanRankDataset,
        method: CandidateRankingMethod,
        dsquery: str,
        for_training: bool = False,
        num_proc: Optional[int] = None,
    ):
        return slugify(
            f"{method.__class__.__name__}_{getattr(method, 'VERSION')}_{dsquery}_{for_training}"
        ).replace("-", "_")


CanRankDataset.run = Cache.cls(MyDatasetDict, None).file(
    cache_key=CanRankDataset.get_cache_key, filename=CanRankDataset.get_cache_filename
)(CanRankDataset.run)
